import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
import joblib
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score
from sklearn.model_selection import LeaveOneGroupOut
import pandas as pd
from sklearn.preprocessing import StandardScaler
import os

def evaluate_ml_model_lopo(model_path, scaler_path, X, y, pids, log_path):
    """Evaluate ML models with LOPO-CV."""
    
    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)

    logo = LeaveOneGroupOut()
    auc_scores = []
    
    for train_idx, test_idx in logo.split(X, y, groups=pids):
        print(f"ðŸ” Evaluating participant {pids[test_idx][0]}")

        X_test, y_test = X[test_idx], y[test_idx]
        X_test_scaled = scaler.transform(X_test)

        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        auc = roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else 0.5
        auc_scores.append(auc)

    print(f"âœ… Mean AUC: {np.mean(auc_scores):.4f} Â± {np.std(auc_scores):.4f}")

def evaluate_lstm_lopo(model_path, scaler_path, X, y, pids, log_path):
    """
    Evaluate an LSTM model using LOPO-CV and log AUROC, Accuracy, and Precision.
    """

    # âœ… Load Model and Scaler
    model = tf.keras.models.load_model(model_path)
    scaler = StandardScaler()

    # âœ… Ensure pids matches X by trimming `pids` to match `X`
    if len(pids) != len(X):
        print(f"âš ï¸ Adjusting pids from {len(pids)} to {len(X)} to match sequences")
        
        # Trim `pids` if it's longer than `X`
        if len(pids) > len(X):
            pids = pids[:len(X)]
        else:
            # Expand pids by repeating until it matches X
            pids = np.repeat(pids, len(X) // len(pids) + 1)[:len(X)]

    # âœ… Confirm final shapes
    print(f"âœ… Final X shape: {X.shape}")  # Expect (190295, 10, 14)
    print(f"âœ… Final y shape: {y.shape}")  # Expect (190295,)
    print(f"âœ… Final pids shape: {pids.shape}")  # Expect (190295,)

    assert len(pids) == len(X), f"âŒ ERROR: `pids` length {len(pids)} != `X` length {len(X)}"

    # âœ… LOPO-CV Setup
    logo = LeaveOneGroupOut()
    auc_scores = []
    accuracy_scores = []
    precision_scores = []
    participant_results = []

    for train_idx, test_idx in logo.split(X, y, groups=pids):
        test_pid = np.unique(pids[test_idx])[0]
        print(f"ðŸ” Evaluating on participant {test_pid}")

        # âœ… Test Set
        X_test, y_test = X[test_idx], y[test_idx]

        if X_test.shape[0] == 0:
            print(f"âš ï¸ Skipping participant {test_pid} due to insufficient test samples.")
            continue

        # âœ… Standardization
        X_test_scaled = scaler.fit_transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

        # âœ… Predictions
        y_pred_proba = model.predict(X_test_scaled).flatten()
        y_pred = (y_pred_proba >= 0.5).astype(int)

        # âœ… Metrics
        auc = roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else 0.5
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)

        auc_scores.append(auc)
        accuracy_scores.append(accuracy)
        precision_scores.append(precision)

        # âœ… Store Results
        for actual, predicted in zip(y_test, y_pred):
            participant_results.append([test_pid, actual, predicted])

        print(f"âœ… Participant {test_pid}: AUC={auc:.4f}, Accuracy={accuracy:.4f}, Precision={precision:.4f}")

    # âœ… Compute Mean and Std
    mean_auc = np.mean(auc_scores)
    std_auc = np.std(auc_scores)
    mean_acc = np.mean(accuracy_scores)
    std_acc = np.std(accuracy_scores)
    mean_precision = np.mean(precision_scores)
    std_precision = np.std(precision_scores)

    # âœ… Convert results to DataFrame
    participant_df = pd.DataFrame(participant_results, columns=["Participant", "Actual", "Predicted"])

    # âœ… Log Results
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    with open(log_path, 'w') as f:
        f.write(f"LOPO-CV AUROC: {mean_auc:.4f} Â± {std_auc:.4f}\n")
        f.write(f"LOPO-CV Accuracy: {mean_acc:.4f} Â± {std_acc:.4f}\n")
        f.write(f"LOPO-CV Precision: {mean_precision:.4f} Â± {std_precision:.4f}\n\n")
        f.write("ðŸ“Š Per-Participant Results:\n")
        f.write(participant_df.to_string(index=False))

    print(f"âœ… LOPO-CV AUROC: {mean_auc:.4f} Â± {std_auc:.4f}")
    print(f"âœ… LOPO-CV Accuracy: {mean_acc:.4f} Â± {std_acc:.4f}")
    print(f"âœ… LOPO-CV Precision: {mean_precision:.4f} Â± {std_precision:.4f}")

    return mean_auc, std_auc, mean_acc, std_acc, mean_precision, std_precision
